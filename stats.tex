\chapter{Statistical methods}
\label{ch:stat}

In experimental particle physics, results are typically extracted by directly comparing detector-level predictions, for example obtained using MC simulation as discussed in the previous chapter, to the observed data for suitably chosen observables. The measured data here are necessarily afflicted by statistical uncertainties, both due to the probabilistic nature of quantum mechanics and the inherent randomness of particle interactions in the detector. They should thus be seen as a sample drawn from a random distribution, and in order to extract underlying parameters of any model, statistical methods are required.

In this work, all statistical interpretation is performed in the framework of \textit{binned profile maximum likelihood fits}. This method follows the Frequentist approach to statistics, in which probability is seen purely as a measure of the \textit{frequency} for a random event to occur when a random experiment is repeated many times.
Thus, physical properties that should be extracted are considered to be fixed, if unknown, quantities, which enter the random distribution of the observed data as parameters. In order to estimate the desired properties, the observed data points are sorted into orthogonal bins according to one or more sensitive observables, and each bin is treated as an independent counting experiment where the observed number of events is given by a Poisson distribution. 

\section{Likelihood definition}
Denoting the set of physical properties to be estimated (the \textit{parameters of interest} or POIs) collectively as $\vec{\mu}$, the likelihood of $\vec{\mu}$ for bin $i$, given that $N_i$ events were observed, is~\cite{Cowan:2010js}

\begin{equation}
    L_i (\vec{\mu}, \vec{\theta}) = \mathrm{Pois} \left(N_i | n_i (\vec{\mu}, \vec{\theta}) \right).
\end{equation}

Here, $\mathrm{Pois}$ refers to the Poisson distribution, and $n_i (\vec{\mu}, \vec{\theta})$ is the mean expected number of events in bin $i$ as predicted by the physics model under consideration. The set of parameters $\vec{\theta}$ are \textit{nuisance parameters} (NPs), which encode the effects of different sources of systematic uncertainty affecting the measurement. The full likelihood of the measurement is now given as the product of all bins:

\begin{equation}
    L (\vec{\mu}, \vec{\theta}) = \prod_i L_i (\vec{\mu}, \vec{\theta}) \cdot G(\vec{\theta}).
\end{equation}

The function $G(\vec{\theta})$ represents the \textit{constraint terms} of the NPs, encoding any possible prefit uncertainties on them. For example, an experimental source of uncertainty (e.g. an efficiency) $f$ might be measured with a mean value of $\hat{f}$ and standard deviation $\Delta f$. Then, the corresponding NP would be normalized as $\theta_f = (f-\hat{f})/\Delta f$, and the constraint terms $G(\vec{\theta})$ would include a factor $\mathcal{N}(\theta_f | \, 0,1)$, i.e. the standard normal distribution for $\theta_f$. This way, the range $\theta_f = \pm 1$ corresponds to one standard deviation of the corresponding systematic uncertainty source.

In practice, the functional form of the expectation $n_i$ must be given by the physics model studied in the experiment. In this work, it is modeled as a sum of signal and background processes. An important case is a linear signal, where the only POI is the \textit{signal strength} $\mu$ and the expectation for bin $i$ is

\begin{equation}
\label{eq:methods:linearsignal}
    n_i (\mu, \vec{\theta}) = \mu s_i (\vec{\theta}) + b_i (\vec{\theta}).
\end{equation}

The functions $s_i$ and $b_i$ are the signal and background expectations, respectively, which both can be influenced by NPs. 

To extract a best-fit value of the POI (or multiple POIs), one now maximizes the full likelihood simultaneously over both the POIs $\vec{\mu}$ and the NPs $\vec{\theta}$, giving the \textit{maximum likelihood estimator} for $\vec{\mu}$. In practice, usually the function $-2 \ln L$ is minimized instead to have numerically tractable quantities. 

\section{Confidence intervals}

In the Frequentist approach to statistics, an uncertainty can be assigned to the estimate in the form of \textit{confidence intervals}. To do so, a \textit{test statistic} has to be defined, which usually takes the form of a \textit{profile likelihood ratio}, e.g.~\cite{Cowan:2010js}

\begin{equation}
\label{eq:methods:teststat}
    \lambda (\vec{\mu}) = - 2 \ln \frac { \hat{L} (\vec{\mu}) } { \max_{\vec{\mu}^{\,\prime}} \hat{L} (\vec{\mu}^{\,\prime}) } 
    \quad \text{with} \quad 
    \hat{L} (\vec{\mu}) = \max_{\vec{\theta}} L (\vec{\mu}, \vec{\theta}).
\end{equation}

$\hat{L} (\vec{\mu})$ is the profile likelihood, i.e. the likelihood maximized over the NPs, and the ratio is taken between the probed POI values $\vec{\mu}$ and the best-fit values $\vec{\mu}^{\,\prime}$. Small values of $ \lambda (\vec{\mu})$ now signalize good agreement with the data for the POI values $\vec{\mu}$. The value of the test statistic $\lambda$ depends on the observed data $N_i$, and can thus be seen as a random variable with a probability density $f(\lambda | \vec{\mu})$, which again depends on the POIs as parameters. Then, given an observed value of the test statistic $\lambda^{\mathrm{obs}}$, a set of POIs $\vec{\mu}$ is excluded at confidence level (CL) $\alpha$ if 

\begin{equation}
\label{eq:methods:cl}
    \mathcal{P} \left( \lambda (\vec{\mu}) < \lambda^{\mathrm{obs}} \right) = \int_0^{\lambda^{\mathrm{obs}}} f(\lambda | \vec{\mu}) \, d\lambda > \alpha.
\end{equation}

The probability density $f(\lambda | \vec{\mu})$ can be evaluated numerically using toy data sets. Alternatively, for simple signal models like the linear signal given in \cref{eq:methods:linearsignal}, it can be analytically shown that $\lambda$ is approximately $\chi^2$-distributed, with the degrees of freedom equaling the number of POIs~\cite{Wilks:1938dza,Wald:1943}.%By using the analytical cumulative distribution function of the $\chi^2$ distribution, and

In particular, for the case of one POI $\mu$ with best-fit value $\hat{\mu}$, a two-sided confidence interval at $\sim68\%$ CL (corresponding to one standard deviation of the normal distribution) is then simply given as~\cite{Cowan:2010js}

\begin{equation}
    \lambda(\hat{\mu} \pm \Delta \mu) = -2 (\ln \hat{L} (\hat{\mu} \pm \Delta \mu) - \ln \hat{L} (\hat{\mu})) = 1.
\end{equation}

That is, the uncertainty corresponds to a change in the negative profile log-likelihood $-2 \ln \hat{L}$ by 1 with respect to the best-fit point.

\paragraph{Significance}

The framework of confidence intervals can also be used to define the \textit{significance} of an observed signal. To do so, a hypothesis test is performed, with the background-only case as the null hypothesis to be rejected. For an observed value of the test statistic $\lambda^{\mathrm{obs}}$ (defined again by \cref{eq:methods:teststat}), the probability to make this observation under the background-only hypothesis (the \textit{$p$-value}) is

\begin{equation}
\label{eq:methods:pvalue}
    p_0 = \int_0^{\lambda^{\mathrm{obs}}} f(\lambda | \vec{\mu} = 0) \, d\lambda.
\end{equation}

To translate this into a significance, the $p$-value is compared to the area under the curve of a standard normal distribution: a significance of 2 standard deviations, giving $\approx 95\%$ probability under the normal distribution, corresponds to a $p$-value of $0.05$. Similar to the case described above, the $p$-value can be obtained from analytical approximate distributions in the case of a simple linear signal.

\paragraph{Exclusion limits}

A different application of confidence intervals are \textit{exclusion limits}, used in experiments where no or little signal was observed. Here, for a POI that is bounded from below (usually by zero, e.g. a signal strength), an upper limit $\mu^{\mathrm{up}}$ is sought such that all values $\mu > \mu^{\mathrm{up}}$ are excluded at a certain CL. At the LHC, the \CLs method~\cite{Junk:1999kv,Read:2002hq} is commonly used for this purpose. The test statistic is modified from \cref{eq:methods:teststat} to be 

\begin{equation}
    q(\mu) = \begin{cases} 
        \lambda(\mu), & \hat{\mu} \leq \mu \\
        0, & \hat{\mu} > \mu
    \end{cases}
\end{equation}

\noindent where $\hat{\mu}$ again refers to the best-fit value of $\mu$. The point of this modification is that a certain value of $\mu$ should not be seen as excluded if the data is more compatible with a higher $\mu$ value; thus, the test statistic is set to zero in this case.

Following that, for an observed test statistic $q^{\mathrm{obs}}$, the \CLs value is defined as

\begin{equation}
\label{eq:methods:cls}
    \CLs (\mu) = \frac { p_{\mathrm{s+b}} (\mu) } { 1 - p_{\mathrm{b}} }
\end{equation}

\noindent with

\begin{equation}
    p_{\mathrm{s+b}} (\mu) = \int_0^{q^{\mathrm{obs}}} f(q | \mu) \, dq
    \quad \text{and} \quad
    p_{\mathrm{b}} = \int_0^{q^{\mathrm{obs}}} f(q | \mu = 0) \, dq.
\end{equation}

$p_{\mathrm{s+b}}$ and $p_{\mathrm{b}}$ are the probabilities to observe a test statistic of $q^{\mathrm{obs}}$ under the signal+background and background-only hypotheses, respectively, defined similarly as in \cref{eq:methods:pvalue}. The ratio of the two probabilities is used instead of $p_{\mathrm{s+b}}$ directly to prevent exclusion of small signals in the case that the data is not well compatible with neither the background-only or the signal+background hypothesis (particularly if the experiment is not very sensitive to a certain kind of signal and $p_{\mathrm{s+b}}$ and $p_{\mathrm{b}}$ are thus similar). The exclusion limit at CL $\alpha$ is then simply given by $\CLs (\mu^{\mathrm{up}}) = 1 - \alpha$. A common choice, used in this work, is $\alpha = 95\%$ (corresponding to a $p$-value of 0.05).

\section{Nuisance parameter diagnostics}

Real maximum likelihood fits used in analyses at the LHC are often very complex, with more NPs than there are bins. In such under-constrained fits, the behavior of the different NPs - encoding the different sources of uncertainty - is often not intuitively clear \textit{a priori}, and it is thus important to investigate their postfit behavior to check whether the fit is healthy and numerically stable.

\paragraph{Pulls and constraints}
To do so, first, the \textit{pull} of a given NP $\theta$ is defined as

\begin{equation}
    P_\theta = \frac{\hat{\theta} - \theta_0} {\Delta \theta_0}
\end{equation}

\noindent where $\hat{\theta}$ is the best-fit value of the NP, $\theta_0$ is the prefit value, and $\Delta \theta_0$ is the prefit uncertainty.  To have an NP pulled thus means that its best-fit value is different from the prefit expectation. Similarly, the \textit{constraint} of the NP is defined as

\begin{equation}
    C_\theta = \frac{\Delta \hat{\theta}} {\Delta \theta_0}
\end{equation}

\noindent where $\Delta \hat{\theta}$ is the estimated postfit uncertainty of the NP, defined in the same fashion as for the POIs above.\footnote{In case the NP is normalized to a standard normal distribution as defined above, one has simply $P_\theta = \hat{\theta}$ and $C_\theta = \Delta \hat{\theta}$.} To have an NP constrained means that its estimated postfit uncertainty is smaller than the assumed prefit value.

Both of these effects are not necessarily a sign of an unhealthy fit: if the observables considered in the fit are sensitive to a particular physical or experimental parameter as encoded by the NP in question, a constraint, and possibly a pull, are expected, and simply show the power of the fit to measure that particular parameter.

If, on the other hand, a strong constraint or pull (beyond what is expected from statistical fluctuations) is seen in an NP to which no sensitivity is expected, it might be a sign of problems with the fit, such as spurious constraints from noisy inputs, missing degrees of freedom to describe the data, or too small prefit uncertainties. Whether this casts doubt on the result or not needs to be gauged on a case-by-case basis, and depends on the relevance of the NP in question.

The relevance of individual NPs to the result can be quantified using \textit{impacts}. The impact of an NP $\theta$ with best-fit value $\hat{\theta}$ and postfit uncertainty $\Delta \hat{\theta}$ is defined by repeating the maximum likelihood fit at values of $\hat{\theta} \pm \Delta \hat{\theta}$, with $\theta$ then held fixed in the maximization of \cref{eq:methods:teststat}. The shift in the resulting POI values with respect to the best-fit POI is the impact on that particular POI. In a fit with a single POI, the impacts can be used to rank the NPs and the systematic uncertainties they encode in order of importance to the fit result. In particular, NPs with very small impact can be considered irrelevant for the fit result. However, it should be kept in mind that this procedure does not fully account for possible correlations between the NPs.

\paragraph{Uncertainty breakdown} 

Related but not identical to the concept of impacts is an \textit{uncertainty breakdown}, which can be used to quantify the contribution from different sources of uncertainty to the total postfit uncertainty on the POI. To do so, either a single NP or a group of NPs originating from the same source (e.g. all NPs corresponding to a certain correction) are frozen at their postfit values, and the fit is repeated with the POI and remaining parameters left untouched. The result will yield the same best-fit value for the POI, but with a possibly reduced uncertainty (as estimated from the likelihood). The uncertainty due to the frozen NP or NP group is then defined as the quadratic difference to the nominal uncertainty.
This method does not account for correlations between different uncertainty sources (though it does consider correlations between the NPs in a certain group). As a result, the uncertainties obtained in this way will in general not sum up in quadrature to the original uncertainty. 

A further use of this method is to define the statistical component of the uncertainty on a POI: it is simply the remaining uncertainty when all considered NPs are frozen to their postfit values simultaneously. Conversely, the quadratic difference between the total and the statistical uncertainty can be considered the systematic uncertainty.

\section{Technical implementation}
In this work, two different tools are used to implement the methods described above. In \cref{ch:ttxs,ch:ah}, where experimental data is analyzed, the CMS general-purpose statistics tool \texttt{combine} is used~\cite{CMS:CAT-23-001}. In \cref{ch:alps}, on the other hand, the Python-based tool \texttt{pyhf}~\cite{pyhf_joss} is employed for the purpose of calculating expected significances and limits.

%The uncertainty in the best-fit value can be estimated by \textit{profiling} the likelihood over the nuisance parameters. That is, one defines the profile likelihood as 

%\begin{equation}
%    \hat{L} (\vec{\mu}) = \max_{\vec{\theta}} L (\vec{\mu}, \vec{\theta})
%\end{equation}

%\noindent and 