\chapter{Experimental methods}
\label{ch:methods}

\section{The Large Hadron Collider}

At the time of writing, the Large Hadron Collider~\cite{Bruning:2004ej} is the largest and most powerful particle accelerator in the world. Located underground at the border of France and Switzerland close to Geneva, it consists of two circular beamlines of roughly 27~km circumference, in which proton bunches are accelerated and collided. Superconducting magnets, cooled with liquid helium at around 4~K temperatures, generate magnetic fields of over 8~T to keep the protons on their circular orbit, and similarly superconducting electromagnetic radio-frequency cavities accelerate the protons to beam energies up to 7~TeV. When operating as designed, around 2800 proton bunches per beam containing $3\times10^{14}$ protons total are present in the beamline simultaneously, revolving with a frequency of about 11.245~kHz. From this, peak instantaneous luminosities of about \SI{20}{\kilo\hertz\per\micro\barn} can be reliably reached. Alternatively, the LHC can also collide heavy ions, such as lead or oxygen, instead of protons.

There are four large experiments making use of the colliding beams at the LHC, located at the four interaction points. The two larger of these are ATLAS~\cite{ATLAS:2008xda} and CMS~\cite{CMS:2008xjf}, both of which are general-purpose experiments intended to study all aspects of the Standard Model in proton-proton collisions. The work of thesis was performed as part of the CMS collaboration, and so the CMS experiment is described in \cref{sec:methods:cms} in more detail. The two smaller experiments, on the other hand, are specialized for certain tasks, namely the study of B physics and exotic hadrons for LHCb~\cite{LHCb:2008vvz} and the study of heavy-ion collisions for ALICE~\cite{ALICE:2008ngc}.

The data taken at the LHC so far can be divided into three Runs. Run~1 lasted from 2010--2012, during which the LHC operated at center-of-mass energies of 7 and 8~TeV, significantly below the original target values, and yielded a total integrated luminosity of about \SI{29}{\fbinv}. It is this data that led to the original discovery of the Higgs boson. Following this, after two years of pause, Run~2 resumed in 2015 with a center-of-mass energy of 13~TeV and lasting to 2018. Around \SI{140}{\fbinv} of data was collected during this time. This complete data set, save for the small contribution from 2015, is analyzed in \cref{ch:ah} of this thesis.

Finally, Run~3 of the LHC started in 2022 after another three years of pause, and is planned to last until 2026 at the time of writing. The center-of-mass energy was again increased slightly to 13.6~TeV, and in the years 2022--2024 around \SI{196}{\fbinv} have been recorded, already surpassing Run~2. In \cref{ch:ttxs} of this thesis, the very first data of Run~3, corresponding to \SI{1.21}{\fbinv} taken in July and August 2022 at CMS, are analyzed in the context of a \ttbar cross section measurement. 

In the future, it is planned to upgrade the LHC to be able to run at higher instantaneous luminosities as well as a further increased energy of 14~TeV~\cite{ZurbanoFernandez:2020cco}. The CMS detector will similarly be upgraded to replace aging components and deal with the increased pileup conditions~\cite{CMS:TDR-15-02,CMS:PRF-21-001}, and a total integrated luminosity of around \SI{3}{\abinv} is expected to be collected. In \cref{ch:alps}, among others, sensitivity projections for this luminosity are made for Axion-Like Particles decaying to \ttbar.

\section{The CMS experiment}
\label{sec:methods:cms}

The Compact Muon Solenoid experiment~\cite{CMS:2008xjf,CMS:PRF-21-001}, located at Interaction Point~5 of the LHC close to Cessy, France, is a general-purpose particle detector targeting a broad range of SM and BSM phenomena. Its main feature is a superconducting solenoid magnet creating a strong magnetic field of 3.8~T. CMS is a hermetic detector, covering almost the full solid angle in space, and is split into a \textit{barrel}, covering pseudorapidities of $\abseta \lesssim 1.5$, and two forward \textit{endcaps}, covering high \abseta values. It consists of several subdetectors, which are geared towards different particle types and properties.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/cms_slice.png}
    \caption{\textbf{The CMS detector}. A cross section view of the different CMS subdetectors, with the trajectories of example particles and their interactions. \textit{Figure taken from \citere{CMS:detector_slice}}.}
\end{figure}

\paragraph{Subdetectors}
The innermost part of CMS is the \textit{tracker}, which is a silicon detector comprised of several layers of silicon pixel and strip sensors~\cite{CMS:2014pgm,CMSTrackerGroup:2020edz}. These record interactions with particles (``tracker hits'') shooting outwards from the interaction point in the center in three-dimensional space. Through reconstruction of the particle tracks and fits of the curvature due to the magnetic field, the tracker thus allows for the measurement of particle momenta. Furthermore, extrapolating the tracks back to their origin allows for the determination of the point of interaction, and thus for discrimination between particles arising from different proton-proton interactions. Due to the presence of the beam pipe, the tracker covers only pseudorapidities of $\abseta < 2.5$, enabling high precision momentum determination in this range only.

The second-to-innermost subdetector is the \textit{electromagnetic calorimeter} (ECAL), which is intended to measure the energy of electrons and photons~\cite{CMS:1997ysd,CMS:EGM-17-001}. It consists of transparent lead tungstate cells, in which incoming electrons or photons create electromagnetic showers leading to avalanches of electron-positron pairs and photon radiation. These are then recorded by photo diodes, and the energy of the incoming particle can be reconstructed from the amount of measured photons. Pseudorapidities of $\abseta < 1.48$ and $1.65 > \abseta < 3$ are covered for the barrel and the endcaps, respectively. The majority electrons and photons are fully stopped in the ECAL and do not interact with the further subdetectors.

Following the ECAL, and similar in functionality, the \textit{hadronic calorimeter} (HCAL) measures the energy of charged or neutral hadrons~\cite{CMS:1997xji,CMS:2012tda}. It consists of interleaved absorber plates, which initiate hadronic showers through the strong interaction with the nuclei of the material, and scintillators, which transmute the hadronic showers into photons to be detected by photodectectors. The HCAL covers $\abseta < 1.4$ and $1.4 < \abseta < 3$ for the barrel and endcaps, respectively, and additionally features a forward section ranging up to $\abseta < 5$, though the latter is not used anywhere in this work.

Surrounding the HCAL lies the superconducting solenoid, followed by the final subdetector: the \textit{muon chambers}~\cite{CMS:1997iti,Pozzobon:2701333}. They are interspersed with four layers of the iron return yoke of the magnet, which confines the magnetic field. Since muons interact only sparsely with matter, they escape the calorimeters and the solenoid unhindered, and are detected in four muon subsystems working in accord at different pseudorapidities: the drift tubes ($\abseta < 1.2$), cathode strip chambers ($0.9 < \abseta < 2.4$), resistive plate chambers ($\abseta < 1.9$) and gas electron multipliers ($1.6 < \abseta < 2.4$). All of them are gas detectors, which are sensitive to the ionization of a gas when a muon passes through it, and record hits of the muon trajectory, thus allowing for a momentum measurement similar to the tracker.

\paragraph{Trigger system}
Besides the different subdetectors, a crucial part of the CMS experiment is the \textit{trigger system}~\cite{CMS-TRG-12-001}. It is necessary due to the large number of bunch crossings at the LHC, which, if they were all recorded, would produce data rates far in excess of the computational bandwidth and storage capacities available. To combat this, only events which are of physical interest should be recorded. It is the task of the trigger system to determine what these events should be.

The trigger system is split into two parts. The first is the low-level or level-one trigger (L1T)~\cite{CMS:TRG-17-001}, which is a hardware trigger consisting of custom electronics and whose inputs are directly the output signals of several of the subdetectors. It is designed to trigger on signatures consistent with specific objects, such as electrons, muons or hadronic jets, with significant energy. Since it needs to take a decision for every collision event, it only has a time interval of around \SI{4}{\micro\second} to do so, requiring purpose-build low-latency electronics. Its target is a output event rate of 100~kHz, which can be adjusted by prescaling certain trigger paths so that only a fraction of passing events is recorded.

The second part of the trigger system is the high-level trigger (HLT)~\cite{CMSTrigger:2005yhe,Varghese:20232Q}. It is a software trigger, running on a GPU-accelerated server farm directly in the CMS service cavern, on which a dedicated, speed-optimized version of the standard CMS object reconstruction algorithm is executed for each event passing the L1T. Specific triggers are then implemented as decisions based on these reconstructed trigger objects, allowing large freedom in selecting events based on the desired physics program. Typical triggers require, for example, the presence of different numbers or combinations of electrons, muons, photons, hadronic jets or missing transverse momentum. The transverse momentum thresholds and further requirements on these objects need to be adjusted so that the total trigger rate is reduced to an average of around 400~Hz. Only these events are then saved to hard drives, and kept for further analysis.

\section{Object reconstruction}
\label{sec:methods:reco}

In order to interpret the physics behind a collision event, the outputs of the subdetectors have to be translated into physics objects which can be mapped to the underlying physical particles. At CMS, this is done with a single unifying method, the Particle Flow (PF) algorithm~\cite{CMS:PRF-14-001}, which is designed to combine the information from the several subdetectors to build physics objects (called PF candidates) as appropriate. The physics objects relevant to this work are listed in the following.

\textit{Charged particle tracks} are obtained from the tracker by fitting recorded tracker hits using a $\chi^2$ minimization, and their momentum and charge are estimated from their curvature as described above~\cite{CMS:2014pgm}. 
%Information from the muon chambers is also included to identify and precisely determine the momenta of muon tracks. 
By extrapolating the tracks back to their origin, the position of vertices in space can also be determined. 
    
From the tracks, the \textit{primary vertices} (PVs) can be determined, which are the locations of the proton-proton interactions that caused the tracks in the first place. By contrast, secondary vertices arise from the decays of particles with long enough lifetime that they move a significant distance from the PV. PVs are determined by a likelihood fit to all tracks of sufficient quality~\cite{CMS:2014pgm}. In each event, the PV whose tracks show the largest \pt sum is designated the hard-scattering PV, assumed to correspond to the physical process of interest, while further PVs are due to soft-QCD pileup interactions. The number of PVs per event is thus a good measure of the amount of pileup.
    
The other main ingredient besides tracks and vertices are \textit{calorimeter clusters} from either the ECAL or the HCAL. A clustering algorithm is required here because particles typically deposit their energy in more than one calorimeter crystal.

By matching the positions of calorimeter clusters and charged particle tracks, \textit{electrons} (for the ECAL) and \textit{charged hadrons} (for the HCAL) can be constructed. The combined measurements of the momentum (from the curvature) and the deposited energy (from the calorimeter) allows for the reconstruction of the mass, and thus the identification of the particle.
For electrons, the effect of bremsstrahlung originating in the tracker volume has to be considered, usually resulting in multiple calorimeter clusters per electron (called a supercluster) which need to be combined together. Isolation criteria on the clusters are also required to veto electrons that are part of a hadronic jet. 
By contrast, calorimeter clusters which do not have charged tracks are assigned to \textit{photons} (for the ECAL) or \textit{neutral hadrons} (for the HCAL). 
CMS furthermore employs algorithms to remove hadrons that are believed to originate from pileup instead of the hard-scattering vertex. In Run~2, the Charged Hadron Subtraction (CHS) method~\cite{CMS:PRF-14-001} was used for this purpose, while in Run~3, the better performing PUPPI method~\cite{Bertolini:2014bba,CMS:2020ebo} was used instead.

\textit{Muons} interact only very rarely with the calorimeter, and are instead built by directly combining charged tracks with hits in the muon chambers. In this work, muons are only considered if they match to hits in both subdetectors.

From these definitions, further high-level objects can be build. The first are \textit{hadronic jets}, which are clustered from all other PF candidates using the anti-$k_T$ algorithm with a distance parameter of $\Delta R = 0.4$~\cite{Cacciari:2008gp} (referred to as AK4 jets). This algorithm is infrared- and collinear-safe, i.e. it is not strongly sensitive to soft nonperturbative QCD effects~\cite{Skands:2012ts}, and has the advantage that the resulting jets are approximately circular in the $\varphi$--$\eta$ plane. Since leptons or photons can be created from electroweak decays of hadrons, these also need to be included in the jet clustering; to ensure that they are not double-counted, leptons and photons that are included in jets are removed from further consideration through isolation criteria.

Hadronic jets can further be \textit{b tagged}, that is, identified as originating from a B hadron. Since the strong interaction is flavor-conserving, the decay of B hadrons to hadrons of other flavors has to be mediated by the flavor-mixing in the weak interaction, leading to comparatively long lifetimes. B hadrons can thus be identified through secondary vertices corresponding to the B hadron decay, which can be displaced from the PV by several millimeters. In practice, machine learning-based classifiers like the \textsc{DeepJet} algorithm~\cite{DeepJet:2020} are used, which take more properties of the jet into account besides the displacement of the secondary vertex.

Finally, the \textit{missing transverse momentum} \ptmissvec can be calculated as the negative of the vectorial sum of all transverse momenta in the event~\cite{CMS:JME-17-001}. Since the initial state of a collision at the LHC has negligible transverse momentum, \ptmissvec represents the total transverse momentum of the particles that left the detector unobserved. In the SM, this is the case for neutrinos, but it could also be BSM particles such as e.g. dark matter candidates.

\section{Statistical interpretation}
\label{sec:methods:stat}

In experimental particle physics, results are typically extracted by comparing detector-level predictions, for example obtained using MC simulation, to the observed data for suitably chosen observables. The measured data here are necessarily afflicted by statistical uncertainties, both due to the inherent randomness of quantum mechanics and the probabilistic behavior of the detector. They should thus be seen as a sample drawn from a random distribution, and in order to extract underlying parameters of any model, statistical methods are required.

In this work, all statistical interpretation is performed in the framework of \textit{binned profile maximum likelihood fits}. This method follows the Frequentist approach of considering physical properties that should be extracted to be fixed, if unknown, quantities, which enter the random distribution of the observed data as parameters. In order to estimate the desired properties, the observed data points are sorted into orthogonal bins according to one or more sensitive observables, and each bin is treated as an independent counting experiment where the observed number of events is given by a Poisson distribution. 

\paragraph{Likelihood definition}
Denoting the set of physical properties to be estimated (the parameters of interest or POIs) collectively as $\vec{\mu}$, the likelihood of $\vec{\mu}$ for bin $i$, given that $N_i$ events were observed, is~\cite{Cowan:2010js}

\begin{equation}
    L_i (\vec{\mu}, \vec{\theta}) = \mathrm{Pois} \left(N_i | n_i (\vec{\mu}, \vec{\theta}) \right).
\end{equation}

Here, $\mathrm{Pois}$ refers to the Poisson distribution, and $n_i (\vec{\mu}, \vec{\theta})$ is the mean expected number of events in bin $i$ as predicted by the physics model under consideration. The set of parameters $\vec{\theta}$ are \textit{nuisance parameters} (NPs), which encode the effects of different sources of systematic uncertainty affecting the measurement. The full likelihood of the measurement is now given as the product of all bins:

\begin{equation}
    L (\vec{\mu}, \vec{\theta}) = \prod_i L_i (\vec{\mu}, \vec{\theta}) \cdot G(\vec{\theta}).
\end{equation}

The function $G(\vec{\theta})$ represents the \textit{constraint terms} of the NPs, encoding any possible prefit uncertainties on them. For example, an experimental source of uncertainty (e.g. a scale factor) $f$ might be measured with a mean value of $\hat{f}$ and standard deviation $\sigma_f$. Then, the corresponding NP would be normalized as $\theta_f = (f-\hat{f})/\sigma_f$, and the constraint terms $G(\vec{\theta})$ would include a factor $\mathcal{N}(\theta_f | \, 0,1)$, i.e. the standard normal distribution for $\theta_f$. This way, the range $\theta_f = \pm 1$ corresponds to one standard deviation of the corresponding systematic uncertainty source.

In practice, the functional form of the expectation $n_i$ must be given by the physics model studied in the experiment. In this work, the events are modeled as a sum of signal and background processes. An important case is a linear signal, where the only POI is the \textit{signal strength} $\mu$ and the expectation for bin $i$ is

\begin{equation}
\label{eq:methods:linearsignal}
    n_i (\mu, \vec{\theta}) = \mu s_i (\vec{\theta}) + b_i (\vec{\theta}).
\end{equation}

The functions $s_i$ and $b_i$ are the signal and background expectations, respectively, which both can be influenced by NPs. 

To extract a best-fit value of the POI (or multiple POIs), one now maximizes the full likelihood simultaneously over both the POIs $\vec{\mu}$ and the NPs $\vec{\theta}$, giving the \textit{maximum likelihood estimator} for $\vec{\mu}$. In practice, usually the function $-2 \ln L$ is minimized instead to have numerically tractable quantities. 

\paragraph{Confidence intervals}

In the Frequentist approach to statistics, an uncertainty can be assigned to the estimate in the form of \textit{confidence intervals}. To do so, a \textit{test statistic} has to be defined, which usually takes the form of a \textit{profile likelihood ratio}, e.g.~\cite{Cowan:2010js}

\begin{equation}
\label{eq:methods:teststat}
    \lambda (\vec{\mu}) = - 2 \ln \frac { \hat{L} (\vec{\mu}) } { \max_{\vec{\mu}^\prime} \hat{L} (\vec{\mu}^\prime) } 
    \quad \text{with} \quad 
    \hat{L} (\vec{\mu}) = \max_{\vec{\theta}} L (\vec{\mu}, \vec{\theta}).
\end{equation}

$\hat{L} (\vec{\mu})$ is the profile likelihood, i.e. the likelihood maximized over the NPs, and the ratio is taken between the probed POI values $\vec{\mu}$ and the best-fit values $\vec{\mu}^\prime$. Small values of $ \lambda (\vec{\mu})$ now signalize good agreement with the data for the POI values $\vec{\mu}$. The value of the test statistic $\lambda$ depends on the observed data $N_i$, and can thus be seen as a random variable with a probability density $f(\lambda | \vec{\mu})$, which again depends on the POIs as parameters. Then, given an observed value of the test statistic $\lambda^{\mathrm{obs}}$, a set of POIs $\vec{\mu}$ is excluded at confidence level (CL) $\alpha$ if 

\begin{equation}
\label{eq:methods:cl}
    P \left( \lambda (\vec{\mu}) < \lambda^{\mathrm{obs}} \right) = \int_0^{\lambda^{\mathrm{obs}}} f(\lambda | \vec{\mu}) \, d\lambda > \alpha.
\end{equation}

The probability density $f(\lambda | \vec{\mu})$ can be evaluated numerically using toy data sets. Alternatively, for simple signal models like the linear signal given in \cref{eq:methods:linearsignal}, it can be analytically shown that $\lambda$ is approximately $\chi^2$-distributed, with the degrees of freedom equaling the number of POIs~\cite{Wilks:1938dza,Wald:1943}.%By using the analytical cumulative distribution function of the $\chi^2$ distribution, and

In particular, for the case of one POI $\mu$ with best-fit value $\hat{\mu}$, a two-sided confidence interval at $\sim 68\%$ CL (corresponding to one standard deviation of the normal distribution) is then simply given as~\cite{Cowan:2010js}

\begin{equation}
    \lambda(\hat{\mu} \pm \Delta \mu) = -2 (\ln \hat{L} (\hat{\mu} \pm \Delta \mu) - \ln \hat{L} (\hat{\mu})) = 1.
\end{equation}

That is, the uncertainty corresponds to a change in negative profile log-likelihood $-2 \ln \hat{L}$ by 1 with respect to the best-fit point.

\paragraph{Significance}

The framework of confidence intervals can also be used to define the \textit{significance} of an observed signal. To do so, a hypothesis test is performed, with the background-only case as the null hypothesis to be rejected. For an observed value of the test statistic $\lambda^{\mathrm{obs}}$ (defined again by \cref{eq:methods:teststat}), the probability to make this observation under the background-only hypothesis (the \textit{$p$-value}) is

\begin{equation}
\label{eq:methods:pvalue}
    p_0 = \int_0^{\lambda^{\mathrm{obs}}} f(\lambda | \vec{\mu} = 0) \, d\lambda.
\end{equation}

To translate this into a significance, the $p$-value is compared to the area under the curve of a standard normal distribution: a significance of 2 standard deviations, giving $\approx 95\%$ probability under the normal distribution, corresponds to a $p$-value of $0.05$. Similar to the case described above, the $p$-value can be obtained from analytical approximate distributions in the case of a simple linear signal.

\paragraph{Exclusion limits}

A different application of confidence intervals are \textit{exclusion limits}, used in experiments where no or little signal was observed. Here, for a POI that is bounded from below (usually by zero, e.g. a signal strength), an upper limit $\mu^{\mathrm{up}}$ is sought such that all values $\mu > \mu^{\mathrm{up}}$ are excluded at a certain CL. At the LHC, the \CLs method~\cite{Junk:1999kv,Read:2002hq} is commonly used for this purpose. The test statistic is modified from \cref{eq:methods:teststat} to be 

\begin{equation}
    q(\mu) = \begin{cases} 
        \lambda(\mu), & \hat{\mu} \leq \mu \\
        0, & \hat{\mu} > \mu
    \end{cases}
\end{equation}

\noindent where $\hat{\mu}$ again refers to the best-fit value of $\mu$. The point of this modification is that a certain value of $\mu$ should not be seen as excluded if the data is more compatible with a higher $\mu$ value; thus, the test statistic is set to zero in this case.

Following that, for an observed test statistic $q^{\mathrm{obs}}$, the \CLs value is defined as

\begin{equation}
\label{eq:methods:cls}
    \CLs (\mu) = \frac { p_{\mathrm{s+b}} (\mu) } { 1 - p_{\mathrm{b}} }
\end{equation}

\noindent with

\begin{equation}
    p_{\mathrm{s+b}} (\mu) = \int_0^{q^{\mathrm{obs}}} f(q | \mu) \, dq
    \quad \text{and} \quad
    p_{\mathrm{b}} = \int_0^{q^{\mathrm{obs}}} f(q | \mu = 0) \, dq.
\end{equation}

$p_{\mathrm{s+b}}$ and $p_{\mathrm{b}}$ are the probabilities to observe a test statistic of $q^{\mathrm{obs}}$ under the signal+background and background-only hypotheses, respectively, defined similarly as in \cref{eq:methods:pvalue}. The ratio of the two probabilities is used instead of $p_{\mathrm{s+b}}$ directly to prevent exclusion of small signals in the case that the data is not well compatible with neither the background-only or the signal+background hypothesis (particularly if the experiment is not very sensitive to a certain kind of signal and $p_{\mathrm{s+b}}$ and $p_{\mathrm{b}}$ are thus similar). The exclusion limit at CL $\alpha$ is then simply given by $\CLs (\mu^{\mathrm{up}}) = 1 - \alpha$. A common choice, used in this work, is $\alpha = 95\%$ (corresponding to a $p$-value of 0.05).

\paragraph{Nuisance parameter diagnostics}

Real maximum likelihood fits used in analyses at the LHC are often very complex, with more NPs than there are bins. In such under-constrained fits, the behavior of the different NPs - encoding the different sources of uncertainty - is often not intuitively clear a priori, and it is thus important to investigate their postfit behavior to check whether the fit is healthy and numerically stable.

To do so, first, the \textit{pulls and constraints} of the NPs are defined as their best-fit values and profiled postfit uncertainties, similar as for the POIs above, relative to their prefit uncertainties. To have a NP pulled means that its best-fit value is different from the prefit expectation. Similarly, to have it constrained means that its estimated postfit uncertainty is smaller than the assumed prefit value. Both of these effects are not necessarily a sign of an unhealthy fit: If the observables considered in the fit are sensitive to a particular physical or experimental parameter as encoded by the NP in question, a constraint, and possibly a pull, are expected, and simply show the power of the fit to measure that particular parameter.

If, on the other hand, a strong constraint or pull (beyond what is expected from statistical fluctuations) is seen in a NP to which no sensitivity is expected, it might be a sign of problems with the fit, such as spurious constraints from noisy inputs, missing degrees of freedom to describe the data, or too small prefit uncertainties. Whether this casts doubt on the result or not needs to be gauged on a case-by-case basis, and depends on the relevance of the NP in question.

The relevance of individual NPs to the result can be quantified using \textit{impacts}. The impact of a NP $\theta$ with best-fit value $\hat{\theta}$ and postfit uncertainty $\Delta \theta$ is defined by repeating the maximum likelihood fit at values of $\hat{\theta} \pm \Delta \theta$, with $\theta$ then held fixed in the maximization of \cref{eq:methods:teststat}. The shift in the resulting POI values with respect to the best-fit POI is the impact on that particular POI. In a fit with a single POI, the impacts can be used to rank the NPs and the systematic uncertainties they encode in order of importance to the fit result. In particular, NPs with very small impact can be considered irrelevant for the fit result. However, it should be kept in mind that this procedure does not fully account for possible correlations between the NPs.

\paragraph{Uncertainty breakdown} 

Related but not identical to the concept of impacts is an \textit{uncertainty breakdown}, which can be used to quantify the contribution from different sources of uncertainty to the total postfit uncertainty on the POI. To do so, either a single NP or a group of NPs originating from the same source (e.g. all NPs corresponding to a certain correction) are frozen at their postfit values, and the fit is repeated with the POI and remaining parameters left untouched. The result will yield the same best-fit value for the POI, but with a possibly reduced uncertainty (as estimated from the likelihood). The uncertainty due to the frozen NP or NP group is then defined as the quadratic difference to the nominal uncertainty.
This method does not account for correlations between different uncertainty sources (though it does consider correlations between the NPs in a certain group). As a result, the uncertainties obtained in this way will in general not sum up in quadrature to the original uncertainty. 

A further use of this method is to define the statistical component of the uncertainty on a POI: it is simply the remaining uncertainty when all considered NPs are frozen to their postfit values simultaneously. Conversely, the quadratic difference between the total and the statistical uncertainty can be considered the systematic uncertainty.

\paragraph{Technical implementation}
In this work, two different tools are used to implement the methods described above. In \cref{ch:ttxs,ch:ah}, where experimental data is analyzed, the CMS general-purpose statistics tool \texttt{combine} is used~\cite{CMS:CAT-23-001}. In \cref{ch:alps}, on the other hand, the Python-based tool \texttt{pyhf}~\cite{pyhf_joss} is employed for the purpose of calculating expected significances and limits.

%The uncertainty in the best-fit value can be estimated by \textit{profiling} the likelihood over the nuisance parameters. That is, one defines the profile likelihood as 

%\begin{equation}
%    \hat{L} (\vec{\mu}) = \max_{\vec{\theta}} L (\vec{\mu}, \vec{\theta})
%\end{equation}

%\noindent and 